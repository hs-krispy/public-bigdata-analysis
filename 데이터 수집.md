## 데이터 수집

#### robots.txt

- 크롤링 세계에서 관례적으로 지켜야 하는 규칙 (법적인 효력은 없으나 이에 따라 크롤링을 하는 것이 좋음)
  - User-agent : 디렉티브 정보의 대상이 되는 크롤러
  - Disallow : 크롤링을 거부할 경로
  - Allow : 크롤링을 허가할 경로
  - Sitemap : XML 사이트 맵의 URL
  - Crawl-delay : 크롤러의 간격

<img src="https://user-images.githubusercontent.com/58063806/125195067-fc3cab80-e28e-11eb-91e7-e366bee76651.png" width=25% />

> EX)
>
> User-agent : *
>
> Disallow: /
>
> 모든 크롤러에 대해 모든 페이지의 크롤링을 거부
>
> User-agent : *
>
> Disallow: /admin/
>
> User-agent : ilifobot
>
> Allow : /
>
> 모든 크롤러에게 /admin/ 아래 경로는 크롤링을 거부하고 ilifobot(특정 크롤러)에게는 모든 페이지의 크롤링을 허가

#### requests

- 브라우저 없이 서버에 리소스를 요청할 수 있는 기능을 제공

- request : 클라이언트에서 서버로 전달하는 메시지
  - **GET, POST**, PUT, DELETE 등 9개의 방법이 존재
- response : 서버에서 클라이언트로 전달하는 메시지

**GET**

- 서버에 요청할 데이터를 모두 Header에 담아서 전달하기 때문에 URL에 요청할 정보를 포함해서 전달

```python
import requests

url = "https://www.google.com"
r = requests.get(url)
print(r)
print(r.status_code)
print(r.text)

# <Response [200]>
# 200
# <!doctype html><html itemscope="" itemtype="http://schema.org/WebPage" lang="ko"><head><meta content="text/html; charset=UTF-8" http-equiv="Content-Type"><meta content="/images/branding/googleg/1x/googleg_standard_color_128dp.png" itemprop="image"> ...
```

- html 원시코드는 수많은 태그와 데이터를 포함하기 때문에 BeautifulSoup 패키지를 사용해서 파싱을 진행한 뒤 원하는 데이터만 추출해서 사용 가능

- HTML 상태코드

  - 200 : 서버가 **요청을 제대로 처리** (성공)

  - 403 : 사용자가 **리소스에 대한 필요 권한을 가지고 있지 않아 요청 거부** (금지)

  - 404 : 서버에 **존재하지 않는 페이지에 대한 요청**을 한 경우 (Not Found)

> 응답코드는 200으로 성공으로 뜨지만 리소스(text)는 비어있는 경우가 있음 
>
> (사이트에 따라 사람이 직접 브라우저를 사용하여 URL을 요청하는 것이 아닌 기계가 직접 리소스를 요청하는 경우 결과를 반환하지 않도록 만들어졌기 때문)
>
> 이 경우는 headers 정보를 추가로 지정
>
> User-Agent 정보는 http://www.useragentstring.com/에서 확인 가능
>
> ```python
> headers = {"User-Agent" : "클라이언트 정보"}
> r = requests.get(url, headers=headers)
> ```

- content 속성으로 바이너리 원문 (바이트 스트림)을 추출 가능

```python
from PIL import Image
from io import BytesIO
import matplotlib.pyplot as plt
import requests

url = "https://www.google.co.kr/images/branding/googlelogo/1x/googlelogo_color_272x92dp.png"
r = requests.get(url)
print(r)
print(r.status_code)

# <Response [200]>
# 200

# 바이트 스트림을 사용하여 이미지로 재구성 후 출력
image = Image.open(BytesIO(r.content))
image.show()
```

<img src="https://user-images.githubusercontent.com/58063806/125196256-280e6000-e294-11eb-9cc6-68aa7d577ba8.png" width=30%/>

```python
import os
import requests

url = "https://www.google.co.kr/images/branding/googlelogo/1x/googlelogo_color_272x92dp.png"
r = requests.get(url)
image_name = os.path.basename(url)

# 디렉토리 생성
image_folder = f"{os.getcwd()}/image"
if not os.path.isdir(image_folder):
    os.mkdir(image_folder)

# 이미지 경로 설정    
image_path = os.path.join(image_folder, image_name)

# image 저장
with open(image_path, "wb") as iw:
    iw.write(r.content)
```



**POST**

- Body(본문)에 질의(query)한 단어를 포함시켜서 전달하기 때문에 URL에 표시되지 않음
  - 외부에 값이 노출되지 않으므로 중요한 개인 정보 데이터를 다루거나, 서버로 전달하는 메시지의 길이가 긴 경우에 사용



### 웹 페이지에서 데이터 추출

- 정규 표현식을 이용한 스크래핑은 HTML을 단순한 문자열로 취급하여 필요한 정보 추출
  - 마크업되지 않은 웹 페이지도 스크래핑 가능
- XML 파서를 이용한 스크래핑은 XML 태그를 분석(파싱)하여 필요한 정보 추출
  - 정규 표현식보다 간단하고 효과적으로 필요한 정보 추출 가능
- HTML을 스크래핑할 때는 HTML 전용 파서가 필요
  - 파이썬 표준 모듈인 html.parser 모듈을 사용하면 HTML 파싱 가능 (복잡한 처리 필요)

#### 정규식(Regex, Regexp)

- 특정 검색 패턴(ASCII or 유니코드 문자의 시퀀스)에 대한 하나 이상의 일치 항목을 검색
  - EX) 주민번호 패턴, 이메일 ...
- 문자열 파싱 및 대체, 형 변환, 스크래핑에 이용
- 다양한 프로그래밍 언어와 텍스트 에디터에 적용 가능

**Anchors**

- **^**The : **The로 시작하는** 모든 문자열을 매칭
  - startswith
- end**$** : **end로 끝나는** 문자열과 매칭
  - endswith
- ^The end$ : The end와 정확하게 일치하는 문자열을 매칭
- roar : roar가 들어있는 모든 문자열과 매칭

**Quantifiers**

- abc***** : ab그리고 **0개 이상의 c를 포함**한 문자열과 매칭 
  - ab로 시작하는 모든 문자열 (c는 없어도 됨)
- abc**+** : ab그리고 **1개 이상의 c를 포함**한 문자열과 매칭
- abc**?** : ab 그리고 **0개 또는 1개의 c를 포함**한 문자열과 매칭
- abc**{2}** : ab 그리고 **2개의 c를 포함**한 문자열과 매칭
- abc{2, } : ab 그리고 2개 이상의 c를 포함한 문자열과 매칭
- abc{2, 5} : ab 그리고 2개 이상 5개 이하의 c를 포함한 문자열과 매칭
- a(bc)* : a 그리고 0개 이상의 bc를 포함한 문자열과 매칭
- a(bc){2, 5} : a 그리고 2개 이상 5개 이하의 bc를 포함한 문자열과 매칭

**OR operator**

- a(b|c) : a 그리고 b 또는 c를 포함한 문자열과 매칭

- a[bc] : a 그리고 b 또는 c를 포함한 문자열과 매칭

**Bracket expressions**

- [abc] : a 또는 b 또는 c를 포함하는 문자열과 매칭
- [a-c] : a 또는 b 또는 c를 포함하는 문자열과 매칭
  - -는 범위를 의미
- [0-9]% : 0이상 9이하 숫자 그리고 %문자를 포함한 문자열과 매칭
- [^a-zA-Z] : 영문이 아닌 문자와 매칭, ^는 부정표현으로 사용

**Character classes**

- \d : 모든 숫자와 일치
  - [0-9]와 동일
- \D : \d와 반대 (숫자가 아닌 것)
  - [^0-9]와 동일
- \n : 줄 바꿈
- \r : 캐리지 리턴
- \s : 공백
- \t : 탭
- \w : 영숫자 문자나 언더바
  - [a-zA-Z0-9_]
- \W : \w와 반대
  - [^a-zA-Z0-9_]

- . : 모든 문자중 하나

**Grouping and capturing**

- a(bc) : 소괄호는 캡쳐 그룹을 생성



```python
import re
from html import unescape

with open('dp.html', encoding='utf-8') as f:
    html = f.read()
    
# re.findall() 메서드를 통해 도서 하나에 해당하는 HTML을 추출
for partial_html in re.findall(r'<td class="left"><a.*?</td>', html, re.DOTALL):
    print(partial_html)
    # <td class="left"><a href="/store/books/look.php?p_code=B4300598719">리눅스 입문자를 위한 명령어 사전</a></td>
    
    # 도서의 URL을 추출
    url = re.search(r'<a href="(.*?)">', partial_html)
    print(url.group(0))
    # 전체 일치부 : <a href="/store/books/look.php?p_code=B4300598719">
    print(url.group(1))
    # 첫 번째 일치부 : /store/books/look.php?p_code=B4300598719
    url = 'http://www.hanbit.co.kr' + url.group(1)
    
    # 태그를 제거해서 도서의 제목을 추출
    # 태그와 그 안에 문자들을 모두 공백으로
    title = re.sub(r'<.*?>', '', partial_html)
    title = unescape(title)
    print('url:', url)
    print('title:', title)
    print('---')
    
    # url: http://www.hanbit.co.kr/store/books/look.php?p_code=B4300598719
	# title: 리눅스 입문자를 위한 명령어 사전
	# ---
    # 생략
```

escape - HTML 엔티티 기호(<, " 등)에 대해 본연의 태그나 스크립트 기능을 제외하고 순수 문자로 사용할 수 있도록 변환 

unescape - escape된 문자열을 원래의 HTML 문자열로 복구

#### RSS

- 뉴스나 블로그 등 업데이트가 빈번한 사이트에서 주로 사용하는 콘텐츠 표현 방식
- 구독자들에게 업데이트된 정보를 용이하게 제공하기 위해 **XML 기반**으로 정보 표현 및 제공

#### JSON

- key-value 형식의 데이터 객체를 저장, 전달하기 위해 텍스트 형식의 개방형 표준 포맷
- 플랫폼 및 프로그래밍 언어 독립적 데이터 포맷
- 데이터 타입
  - 문자열(string)
  - 숫자(number)
  - 객체(JSON object)
  - 배열(array)
  - 불리언(Boolean)
  - null

#### SQLite3

- 파일시스템 기반의 경량 관계형 DBMS
  - embedded 환경에서 주로 사용
- 대용량 데이터 및 트랜잭션 처리 부적합
- 경량 데이터 및 트랜잭션 처리, 교육용 목적 등으로 사용



```python
from xml.etree import ElementTree
import pandas as pd

tree = ElementTree.parse("rss.xml")

# getroot() 메서드로 XML의 루트 element를 추출
# XML 문서는 계층적인 구조를 가짐
root = tree.getroot()

데이터프레임_리스트 = []
for item in root.findall("channel/item/description/body/location/data"):
    # find() 메서드로 element 탐색, text 속성으로 값을 추출
    tm_ef = item.find("tmEf").text
    tmn = item.find("tmn").text
    tmx = item.find("tmx").text
    wf = item.find("wf").text
    데이터프레임 = pd.DataFrame({
        "일시": [tm_ef],
        "최저기온": [tmn],
        "최고기온": [tmx],
        "날씨": [wf],
    })
    데이터프레임_리스트.append(데이터프레임)
날씨정보 = pd.concat(데이터프레임_리스트)
날씨정보
```

<img src="https://user-images.githubusercontent.com/58063806/125287637-227a4e00-e358-11eb-82de-687be52c56ff.png" width=30%/>

```python
# csv file
날씨정보.to_csv('날씨정보.csv', encoding="CP949")

# xlsx file
# ExcelWriter 객체 생성
엑셀 = pd.ExcelWriter('날씨정보.xlsx')
# 날씨정보 데이터프레임을 엑셀
날씨정보.to_excel(엑셀, '.', index=False)
# ExcelWriter 객체 close
엑셀.save()

# json file
날씨정보.to_json('날씨정보.json')

# db file
import sqlite3
from pandas.io import sql
import os

with sqlite3.connect(os.path.join('.','sqliteDB')) as con: # sqlite DB 파일이 존재하지 않는 경우 파일생성
    try:
        날씨정보.to_sql(name='WEATHER_INFO', con=con, index=False, if_exists='append') 
        #if_exists : {'fail', 'replace', 'append'} default : fail
    except Exception as e:
        print(str(e))
    
    query = 'SELECT * FROM WEATHER_INFO'
    데이터프레임1 = pd.read_sql(query, con=con)
```



#### 파이썬 스크래핑 프로세스

1. 웹 페이지 크롤링
   - fetch(url) : 매개변수로 지정한 URL의 웹 페이지를 추출
2. 스크래핑
   - scrape(html) : 매개변수로 html을 받고, 정규 표현식을 사용해 HTML에서 정보 추출

  3. 데이터 저장
     - save(db_path, books) : 매개변수로 books 정보를 받아 db_path로 지정된 SQLite DB에 저장



#### lxml

- C언어로 작성된 XML 처리와 관련된 라이브러리인 libxml2와 libxslt의 파이썬 바인딩
- libxml2와 libxslt는 C언어로 작성되어 있으므로 빠르게 동작
- API
  - lxml.etree : ElementTree를 확장한 API를 가진 XML 파서
  - lxml.html : xml.etree를 기반으로 invalid HTML도 다룰 수 있게 해주는 HTML 파서

#### Beautiful Soup

- 간단하고 이해하기 쉬운 직관적인 API를 활용해 데이터를 추출
- 내부적으로 사용되는 파서를 목적에 맞게 변경 가능
  - html.parser : 표준
  - lxml : 빠른 처리 가능
  - html5lib : html5의 사양에 맞게 파싱
- HTML이나 XML 같은 태그로 구성되어 있는 문서를 파싱하는데 사용

**주요함수**

- find_all(태그) : 태그가 포함되어 있는 모든 문장 반환
- find(태그) : 태그가 포함되어 있는 모든 문장 중 첫 번째 문장 반환
- select(selector) : selector를 사용한 데이터 선택

```python
from bs4 import BeautifulSoup

soup = BeautifulSoup('''
<html>
<head><title>온라인 과일 가게</title></head>
<body>
<h1 id="main">오늘의 과일</h1>
<ul>
    <li>사과</li>
    <li>포도</li>
    <li class="featured">귤</il>
</ul>
</body>
</html>
''', "html.parser")

print(soup.h1)
# <h1 id="main">오늘의 과일</h1>
print(type(soup.h1))
# <class 'bs4.element.Tag'>

soup.h1.name
# 'h1'

# h1 태그 내부의 문자열을 추출
print(soup.h1.string)
# 오늘의 과일
print(type(soup.h1.string))
# <class 'bs4.element.NavigableString'>
 
# ul 태그 내부의 모든 문자열을 결합해서 추출 
print(soup.ul.text)
# 사과
# 포도
# 귤
print(type(soup.ul.text))
# <class 'str'>

# Tag 객체에서 id 속성값 추출
# soup.h1.get("id")와 동일
soup.h1["id"]
# 'main'

soup.h1.attrs
```

- string은 요소 바로 아래의 문자열 추출
- text는 요소 내부의 모든 문자열을 결합해서 추출
  - class 속성이 지정된 문자열 이후에는 추출 안됨
  - string과 text로 추출한 값의 type이 다름
- Tag 객체는 딕셔너리처럼 속성을 추출 가능
- attrs 속성으로 모든 속성을 나타내는 딕셔너리 객체를 추출

```python
# 여러 개의 요소가 있는 경우 가장 앞의 요소를 추출
# soup.find("li")와 동일
soup.li
# <li>사과</li>

# 여러 개의 요소에 대한 리스트를 추출
# soup.findAll("li")와 동일
soup.find_all("li")
# [<li>사과</li>, <li>포도</li>, <li class="featured">귤</li>]

# class 속성에 대한 값을 지정해서 일치하는 객체를 추출
# soup.find_all("li", {"class": "featured"})
# soup.find_all(class_="featured")
soup.find_all("li", class_="featured")
# [<li class="featured">귤</li>]

# tag 이름을 생략하고 id속성 값을 지정해서 일치하는 객체를 추출
soup.find_all(id="main")
# [<h1 id="main">오늘의 과일</h1>]

# 모든 해당 요소 객체들을 반환
soup.select("li")
# [<li>사과</li>, <li>포도</li>, <li class="featured">귤</li>]

# soup.select(".featured")와 동일
soup.select("li.featured")
# [<li class="featured">귤</li>]

soup.select("#main")
# [<h1 id="main">오늘의 과일</h1>]
```



#### URL 구조

`http://example.com/main/index?q=python#lead`

- schema(스키마) : http
  - http, https와 같은 프로토콜
- authority : example.com
  - 도메인 명 : 포트번호
- path : main/index
  - 호스트 내부에서의 리소스 경로
- query : q=python (key, value)
  - "?" 뒤에 나오는 경로와는 다른 방법으로 리소스를 표현하는 방법
- flagment : lead
  - "#" 뒤에 나오는 리소스 내부의 특정 부분

- 절대 URL : https : // 등의 스키마로 시작하는 URL
- 상대 URL : 절대 URL을 기준으로 상대적인 경로를 잡는 URL



urljoin() - 첫 번째 매개변수에 기준이 되는 URL을 지정, 두 번째 매개변수에 상대 URL을 지정

**(상대 URL을 절대 URL로 변환)**

```python
from urllib.parse import urljoin
base_url = "http://example.com/books/top.html"

urljoin(base_url, "//cdn.example.com/logo.png")
# 'http://cdn.example.com/logo.png'

urljoin(base_url, "/articles/")
# 'http://example.com/articles/'
```



#### 퍼머링크(Permalink)

- 불변(Permenant)와 링크(link)를 조합
- 최근의 웹사이트는 하나의 콘텐츠가 하나의 URI에 대응
  - URI : 인터넷에 있는 자원을 나타내는 유일한 주소
- 하나의 콘텐츠에 대응되고, 시간이 흘러도 대응되는 콘텐츠가 변하지 않는 URL이라고 할 수 있음

