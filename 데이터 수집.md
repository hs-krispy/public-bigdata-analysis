## 데이터 수집

#### robots.txt

- 크롤링 세계에서 관례적으로 지켜야 하는 규칙 (법적인 효력은 없으나 이에 따라 크롤링을 하는 것이 좋음)
  - User-agent : 디렉티브 정보의 대상이 되는 크롤러
  - Disallow : 크롤링을 거부할 경로
  - Allow : 크롤링을 허가할 경로
  - Sitemap : XML 사이트 맵의 URL
  - Crawl-delay : 크롤러의 간격

<img src="https://user-images.githubusercontent.com/58063806/125195067-fc3cab80-e28e-11eb-91e7-e366bee76651.png" width=25% />

> EX)
>
> User-agent : *
>
> Disallow: /
>
> 모든 크롤러에 대해 모든 페이지의 크롤링을 거부
>
> User-agent : *
>
> Disallow: /admin/
>
> User-agent : ilifobot
>
> Allow : /
>
> 모든 크롤러에게 /admin/ 아래 경로는 크롤링을 거부하고 ilifobot(특정 크롤러)에게는 모든 페이지의 크롤링을 허가

#### requests

- 브라우저 없이 서버에 리소스를 요청할 수 있는 기능을 제공

- request : 클라이언트에서 서버로 전달하는 메시지
  - **GET, POST**, PUT, DELETE 등 9개의 방법이 존재
- response : 서버에서 클라이언트로 전달하는 메시지

**GET**

- 서버에 요청할 데이터를 모두 Header에 담아서 전달하기 때문에 URL에 요청할 정보를 포함해서 전달

```python
import requests

url = "https://www.google.com"
r = requests.get(url)
print(r)
print(r.status_code)
print(r.text)

# <Response [200]>
# 200
# <!doctype html><html itemscope="" itemtype="http://schema.org/WebPage" lang="ko"><head><meta content="text/html; charset=UTF-8" http-equiv="Content-Type"><meta content="/images/branding/googleg/1x/googleg_standard_color_128dp.png" itemprop="image"> ...
```

- html 원시코드는 수많은 태그와 데이터를 포함하기 때문에 BeautifulSoup 패키지를 사용해서 파싱을 진행한 뒤 원하는 데이터만 추출해서 사용 가능

- HTML 상태코드

  - 200 : 서버가 **요청을 제대로 처리** (성공)

  - 403 : 사용자가 **리소스에 대한 필요 권한을 가지고 있지 않아 요청 거부** (금지)

  - 404 : 서버에 **존재하지 않는 페이지에 대한 요청**을 한 경우 (Not Found)

> 응답코드는 200으로 성공으로 뜨지만 리소스(text)는 비어있는 경우가 있음 
>
> (사이트에 따라 사람이 직접 브라우저를 사용하여 URL을 요청하는 것이 아닌 기계가 직접 리소스를 요청하는 경우 결과를 반환하지 않도록 만들어졌기 때문)
>
> 이 경우는 headers 정보를 추가로 지정
>
> User-Agent 정보는 http://www.useragentstring.com/에서 확인 가능
>
> ```python
> headers = {"User-Agent" : "클라이언트 정보"}
> r = requests.get(url, headers=headers)
> ```

- content 속성으로 바이너리 원문 (바이트 스트림)을 추출 가능

```python
from PIL import Image
from io import BytesIO
import matplotlib.pyplot as plt
import requests

url = "https://www.google.co.kr/images/branding/googlelogo/1x/googlelogo_color_272x92dp.png"
r = requests.get(url)
print(r)
print(r.status_code)

# <Response [200]>
# 200

# 바이트 스트림을 사용하여 이미지로 재구성 후 출력
image = Image.open(BytesIO(r.content))
image.show()
```

<img src="https://user-images.githubusercontent.com/58063806/125196256-280e6000-e294-11eb-9cc6-68aa7d577ba8.png" width=30%/>

```python
import os
import requests

url = "https://www.google.co.kr/images/branding/googlelogo/1x/googlelogo_color_272x92dp.png"
r = requests.get(url)
image_name = os.path.basename(url)

# 디렉토리 생성
image_folder = f"{os.getcwd()}/image"
if not os.path.isdir(image_folder):
    os.mkdir(image_folder)

# 이미지 경로 설정    
image_path = os.path.join(image_folder, image_name)

# image 저장
with open(image_path, "wb") as iw:
    iw.write(r.content)
```



**POST**

- Body(본문)에 질의(query)한 단어를 포함시켜서 전달하기 때문에 URL에 표시되지 않음
  - 외부에 값이 노출되지 않으므로 중요한 개인 정보 데이터를 다루거나, 서버로 전달하는 메시지의 길이가 긴 경우에 사용

#### BeautifulSoup

- HTML이나 XML 같은 태그로 구성되어 있는 문서를 파싱하는데 사용

**주요함수**

- find_all(태그) : 태그가 포함되어 있는 모든 문장 반환
- find(태그) : 태그가 포함되어 있는 모든 문장 중 첫 번째 문장 반환
- select(selector) : selector를 사용한 데이터 선택

