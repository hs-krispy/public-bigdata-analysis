## 웹 크롤링

#### CSSSelect

```python
import requests
import lxml.html

response = requests.get("http://www.hanbit.co.kr/store/books/new_book_list.html")
root = lxml.html.fromstring(response.content)

root.make_links_absolute(response.url)

for a in root.cssselect(".view_box .book_tit a"):
    url = a.get("href")
    print(url)
# https://www.hanbit.co.kr/store/books/look.php?p_code=B7623190015
# https://www.hanbit.co.kr/store/books/look.php?p_code=B4300598719
# https://www.hanbit.co.kr/store/books/look.php?p_code=B9108907099
# ...
```

- fromstring : html코드를 Element라는 클래스 구조로 치환 (묵시적으로 bytes 을 입력으로 하기 떄문에 .content를 사용)
- make_links_absolute : base_url에 존재하는 모든 링크를 절대주소로 변환

#### XPath

```python
import requests
import lxml.html
import pandas as pd
import sqlite3
from pandas.io import sql
import os

REG_DATE = "20200819"

# ?를 기준으로 query string 구분 (? 이후에 나오는 부분이 query string)
response = requests.get(f"https://news.daum.net/breakingnews/digital?regDate={REG_DATE}")
root = lxml.html.fromstring(response.content)
# 어느 element던지 id 값이 mArticle (바뀌지 않는 signature)인 위치를 찾아라
for li in root.xpath('//*[@id="mArticle"]/div[3]/ul/li'):
    a = li.xpath("div/strong/a")[0]
    url = a.get("href")
    print(url, a.text)
# https://v.daum.net/v/20200819230943259 RUSSIA SPACE DOGS BELKA AND STRELKA
# https://v.daum.net/v/20200819230922257 트럼프 "오라클은 대단한 회사"..틱톡 인수 지지
# https://v.daum.net/v/20200819230858252 RUSSIA SPACE DOGS BELKA AND STRELKA
# ...
```

- query string으로 regDate(날짜) 지정



```python
import requests
import lxml.html
import pandas as pd
import sqlite3
from pandas.io import sql
import os
import time

def db_save(NEWS_LIST):
    with sqlite3.connect(os.path.join(".", "sqliteDB")) as con:
        try:
            NEWS_LIST.to_sql(name="NEWS_LIST", con=con, index=False, if_exists="append")
        except Exception as e:
            print(str(e))
        print(len(NEWS_LIST), "건 저장완료..")

def db_delete():
    with sqlite3.connect(os.path.join(".", "sqliteDB")) as con:
        try:
            cur = con.cursor()
            sql = "DELETE FROM NEWS_LIST"
            cur.execute(sql)
        except Exception as e:
            print(str(e))

def db_select():
    with sqlite3.connect(os.path.join(".", "sqliteDB")) as con:
        try:
            query = "SELECT * FROM NEWS_LIST"
            NEWS_LIST = pd.read_sql(query, con=con)
        except Exception as e:
            print(str(e))
        return NEWS_LIST
```

- db_save : 기존에 db 파일이 없으면 새롭게 파일을 생성하고 파일 저장하고 있으면 해당 파일에 추가
- db_delete : db에서 테이블 삭제
- db_select : db에서 원하는 데이터 추출 (위의 경우는 모든 column에 대한 정보)

```python
import re
import string

def get_detail(url):
    body = []
    # string.punctuation에 일부 추가 (특수문자들의 집합)
    punc = '[!"#$%&\'()*+,-./:;<=>?[\]^_`{|}~“”·]'
    response = requests.get(url)
    root = lxml.html.fromstring(response.content)
    # 어느 element던지 id 값이 harmonyContainer (바뀌지 않는 signature)인 위치를 찾아라
    for p in root.xpath('//*[@id="harmonyContainer"]/section/p'):
        if p.text: 
            body.append(re.sub(punc, '', p.text)) # 특수문자 제거
	# body의 모든 문장들을 하나의 문장으로 합침
    full_body = ' '.join(body)
    
    return full_body
```

- get_detail : 상세 페이지들의 url을 받아서 세부 내용을 저장

```python
# 2020년 8월 19일의 58 페이지부터 
page = 58
max_page = 0
REG_DATE = '20200819'

while(True):
    df_list = []
    response = requests.get(f'http://news.daum.net/breakingnews/digital?page={page}&regDate={REG_DATE}')
    root = lxml.html.fromstring(response.content)
    for li in root.xpath('//*[@id="mArticle"]/div[3]/ul/li'):
        a = li.xpath('div/strong/a')[0]
        url = a.get('href')
        article = get_detail(url)
        df = pd.DataFrame({'URL': [url], 'TITLE': [a.text], 'ARTICLE': [article]})
        df_list.append(df)   
        
    if df_list:   
        df_10 = pd.concat(df_list)
        db_save(df_10)

    # 페이지 번호 중에서 max 페이지 가져오기    
    for a in root.xpath('//*[@id="mArticle"]/div[3]/div/span/a'):
        try:
            num = int(a.text)
            if max_page < num:
                max_page = num
        except:
            pass

    # 마지막 페이지 여부 확인 (> 기호 (다음으로 넘어가는 기호) 확인)    
    span = root.xpath('//*[@id="mArticle"]/div[3]/div/span/a[@class="btn_page btn_next"]')

    if (len(span) <= 0) & (page > max_page):
        break
    else:
        page = page + 1
    
    # 잠시 대기
    time.sleep(1)  
    
# 15 건 저장완료..
# 15 건 저장완료..
# 15 건 저장완료..
# 13 건 저장완료..
```

- 현재 페이지부터 마지막 페이지까지 각 페이지들에 있는 기사들의 url, title, article (내용)으로 dataframe을 구성하고 db에 저장 

```python
print(db_select())
#                                        URL  \
# 0   https://v.daum.net/v/20200819084702255   
# 1   https://v.daum.net/v/20200819084617234   
# 2   https://v.daum.net/v/20200819084529215   
# ...

#                                                 TITLE  \
# 0                        에기평, 코로나19 확산 방지 투명칸막이 무상 제공   
# 1                    중국 선전시, 전지역 커버하는 '5G SA' 네트워크 구축   
# 2                   상반기 전세계 모바일게임 지출 42조 돌파..반기 기준 최고   
# ...

#                                        ARTICLE  
# 0   한국에너지기술평가원은 생활방역 정착과 지역경제 활성화에 기여하기 위해 999 비말을...  
# 1   지디넷코리아유효정 중국 전문기자중국 정부 주도로 다음달에서 전국 단위에서 5G 단독...  
# 2   아시아경제 이진규 기자 신종 코로나바이러스감염증코로나19 확산 여파로 실내 활동이 ...  
# ...
```



### Selenium

- 다양한 프로그래밍 언어로 웹드라이버를 통해 다양한 브라우저 상에서 웹 자동화 테스트 or 웹 자동화 프로그램을 구현하기 위한 라이브러리

```python
from selenium.webdriver import Chrome
from selenium.webdriver.common.keys import Keys
from selenium import webdriver
import time
import sqlite3
from pandas.io import sql
import os
import pandas as pd

options = webdriver.ChromeOptions()
# window size 최대화
options.add_argument("--start-maximized");
browser = webdriver.Chrome("chromedriver", options=options)
```

- webdriver를 메모리 상에 load하고 browser를 구동

```python
browser.get("https://www.data.go.kr/")
browser.implicitly_wait(5)
```

- 지정한 url로 이동하고 5초간 대기

```python
browser.find_element_by_xpath('//*[@id="header"]/div/div/div/div[2]/div/a[1]').click()
browser.implicitly_wait(5)

browser.find_element_by_xpath('//*[@id="mberId"]').send_keys("id")
browser.find_element_by_xpath('//*[@id="pswrd"]').send_keys("password")
browser.find_element_by_xpath('//*[@id="loginVo"]/div[2]/div[2]/div[2]/div/div[1]/button').click()
browser.implicitly_wait(5)
```

- 로그인 창으로 이동하는 버튼을 클릭하고 5초간 대기
- id와 password를 입력하는 부분에 각각 사용자가 지정한 id와 password를 넣고 login 버튼 누르고 5초간 대기

```python
browser.find_element_by_xpath('//*[@id="M000400_pc"]/a').send_keys(Keys.ENTER)
browser.find_element_by_xpath('//*[@id="M000402_pc"]/a').send_keys(Keys.ENTER)
```

- 홈페이지에서 원하는 부분을 클릭
  - 홈페이지의 팝업창 때문에 .click() 메서드를 사용하면 error 발생
  - send_keys(Keys.ENTER) 메서드로 변경 후 정상적으로 수행가능

```python
def db_save(ARTICLE_LIST):
    with sqlite3.connect(os.path.join('.','sqliteDB')) as con: # sqlite DB 파일이 존재하지 않는 경우 파일생성
        try:
            ARTICLE_LIST.to_sql(name='ARTICLE_LIST', con=con, index=False, if_exists='append')
        except Exception as e:
            print(str(e))
        print(len(ARTICLE_LIST), '건 저장완료..')
```

- db_save : 기존에 db 파일이 없으면 새롭게 파일을 생성하고 파일 저장하고 있으면 해당 파일에 추가

```python
trs = browser.find_elements_by_xpath('//*[@id="searchVO"]/div[5]/table/tbody/tr')
df_list = []
for tr in trs:
    df = pd.DataFrame({
        "NO": [tr.find_element_by_xpath('td[1]').text],
        "TITLE": [tr.find_element_by_xpath('td[2]').text],
        "IQRY": [tr.find_element_by_xpath('td[3]').text],
        "REGDT": [tr.find_element_by_xpath('td[4]').text],
        "CHGDT": [tr.find_element_by_xpath('td[5]').text]
    })
    df_list.append(df)
    
ARTICLE_LIST = pd.concat(df_list)
db_save(ARTICLE_LIST)
ARTICLE_LIST
```

- 현재 페이지에서 가져오고자하는 정보들을 dataframe으로 만들어서 db에 저장

<img src="https://user-images.githubusercontent.com/58063806/125411885-522d6280-e3f9-11eb-8293-15f2a804bea6.png" width=70% />

```python
browser.find_element_by_xpath('//*[@id="searchVO"]/div[5]/table/tbody/tr[1]/td[2]/a').click()
browser.implicitly_wait(3)

browser.find_element_by_xpath('//*[@id="recsroomDetail"]/div[2]/div[4]/div/a').send_keys(Keys.ENTER)
time.sleep(10)
```

- 원하는 부분으로 이동해서 파일을 다운로드 받을 수도 있음

```python
browser.quit()
```

- 구동되고 있는 browser를 종료시킴
  - quit() 메서드로 종료해야 메모리 할당도 해제되고 완벽히 종료시킬 수 있음 

#### background로 수행

```python
from selenium import webdriver
from selenium.webdriver.common.keys import Keys

options = webdriver.ChromeOptions()
options.add_argument("--headless")
options.add_argument("--disable-gpu")
options.add_argument("--window-size=1280x1024")

browser = webdriver.Chrome('chromedriver', options=options)
```

- 브라우저가 직접 구동되어서 window가 떠있지 않고 background에서 수행
  - headless 옵션에서는 파일을 다운로드 할 때 버그가 있으므로 다운로드가 필요한 경우는 headless 사용 x 

- 위와 동일한 방식으로 진행 결과 모든 코드는 정상적으로 수행되지만 파일 다운로드는 제대로 수행되지 않았음

