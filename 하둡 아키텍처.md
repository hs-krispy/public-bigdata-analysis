## 하둡 아키텍처

### HDFS(Hadoop File System)

- 다수의 서버를 이용하여 클러스터를 구성하고 파일을 분산 저장하여 가용성을 높인 파일 시스템
  - 파일 분할 (Chunk)
  - 서로 다른 노드에 분산 저장 (최소 3 copy, 가용성과 병렬처리(성능) 때문)
  - 파일 업데이트는 불가능 (remove, write만 가능)

#### Write

1. 클라이언트는 네임 노드에게 파일 write 요청
2. 네임 노드는 블록을 저장해야 할 노드 리스트를 알려줌
3. 클라이언트는 데이터 노드와 직접 통신하여 데이터 블록(Chunk) 전송
4. 전송받은 데이터 노드는 블록을 다른 노드로 분산 Copy(총 3개의 사본)
5. Copy 완료 후 Client에게 완료 알림

#### Read

1. 클라이언트는 네임 노드에게 파일 Read 요청
2. 네임 노드는 파일의 Chunk를 갖고 있는 데이터 노드의 리스트를 Client에게 전달
3. 클라이언트는 데이터 노드와 직접 통신하여 데이터 블록(Chunk) 전송 요청
4. 각 데이터 노드는 데이터 블록(Chunk)를 전송하고 Client는 조합하여 요청한 파일 Read



### Map Reduce

- HDFS 저장소를 기반으로 **Key별로 로직을 처리하는 Map 작업**과 **결과를 취합하는 Reduce 작업**으로 구성되어 **병렬 연산** 가능한 하둡 처리기술
- 개발자는 Map 함수와 Reduce 함수 작성만 집중하면 병렬성은 Map Reduce 프레임워크에서 담당
- Task Tracker에는 Slot 이라는 연산단위(CPU/Memory)가 존재하며 Slot은 Map Task 또는 Reduce Task의 고정된 용도로 사전 정의됨

#### Job Tracker

- Client가 제출한 Job을 보고 Map, Reduce Task를 각 노드에게 할당
- job을 스케줄링하고 데이터 노드의 상태를 모니터링

#### Task Tracker

- Job Tracker로부터 Job을 명령받아 Map 연산과 Reduce 연산 수행
- job 수행 결과를 Job Tracker에게 보고

#### 동작 원리

1. Client는 Map Task와 Reduce Task 코드 작성 후 Job Tracker에 업로드
2. Job Tracker는 해당 작업을 각 Task Tracker에게 지시
3. Task Tracker는 작업을 수행하면서 수행 상태를 Job Tracker에게 보고
4. Client가 요청한 작업에 따라 Map Task => Reduce Task => Map Task => Reduce Task 과정이 반복될 수 있으며 각 단계가 종료될 때마다 HDFS에 기록

#### 유의 사항

- 데이터 Skew 
  - 모든 Map 함수가 완료된 이후에 Reduce 함수가 수행됨
  - 특정 Map 함수에 처리할 데이터가 많아 지연되면 전체가 지연
- 파일 개수 및 사이즈
  - 하둡은 대용량 파일을 다루기에 적합하며 파일을 분할 저장하는 Chunk의 크기는 64MB 이상을 권고
- Map 및 Reduce Task의 개수
  - 별도로 설정하지 않으면 Job당 Reduce Task는 1개만 뜨게 됨

#### 한계점

- 네임 노드
  - SPOF - 단일 네임노드 구조로 장애 발생시 전체 클러스터 다운
  - 성능 병목 - 자원관리와 Task 스케줄링까지 모두 담당하여 성능 병목 발생
- 데이터 노드
  - 확장성 한계 - 클러스터당 최대 4천 노드까지 확장에 한계
  - 성능 제한 - 고정적인 Slot 할당으로 자원에 여유가 있어도 Task가 수행되지 못할 수 있음
- 아키텍처
  - 아키텍처 사용성 한계 - Map Task와 Reduce Task라는 제한적인 Job만 수행 가능. 



### 하둡 2.0

- YARN을 도입하여 **리소스 관리와 잡 관리를 분리**시키고 다양한 하둡 생태계 기술을 수용한 하둡 프레임워크
- Map Reduce 외 다양한 어플리케이션 수용가능
- 하둡 1.0 대비 더 많은 Job을 효율적으로 수행

#### 구성요소

- 마스터 노드
  - 리소스 매니저 - 클러스터에서 사용 가능한 전체 자원을 관리, 어플리케이션 마스터가 자원을 요청했을 때 **자원을 할당** (컨테이너)
- 워커 노드
  - 노드 매니저 
    - 각 노드마다 존재하며 하둡 1.0에서 Task Tracker와 비슷한 역할을 수행
    - 노드 내의 자원과 컨테이너를 관리하여 리소스 매니저에게 보고
  - 어플리케이션 마스터
    - 사용자가 Job 처리를 요청시 Job을 관리하기 위해 컨테이너에 구동된 객체
    - **Job 마다 하나의 어플리케이션 마스터가 실행되어 Job을 관리**하게 되며 Job이 완료되면 어플리케이션 마스터도 종료
  - 컨테이너
    - Job 처리를 위해 어플리케이션 마스터가 요청하여 생성되는 자원 단위 (JVM)
    - 한 개의 Job 요청시 여러 개의 컨테이너가 생성됨
    - 하둡 1.0의 Slot과 비슷한 개념으로 하둡 1.0에서는 Map 용도 or Reduce 용도로 고정되어 있었으나 하둡 2.0에서는 용도가 고정되지 않음

<img src="https://user-images.githubusercontent.com/58063806/130403205-f52d2b42-114f-4b65-8658-6b7ca9a3409f.png" width=80% />