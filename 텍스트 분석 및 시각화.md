### 텍스트 분석 및 시각화

- 문장 => 단어 => 키워드 => 필터링(필요없는 글자들 제거) => 집계 => 시각화
  - 문장을 공백 단위로 분할해서 단어로 구성
  - 사전과 대입해서 키워드 추출
  - 필요 없는 단어 필터링
  - 결과 집계
  - 시각화

**불용어 필터링 방법**

- 메모장에 불용어 정리
- 글자 수 지정
  - 띄어쓰기가 되지않은 글은 전체 문장이 하나의 단어로 인식됨

#### 한글

- konlpy 라이브러리 이용

```python
from konlpy.tag import Kkma
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter
import numpy as np
kkma = Kkma()
plt.rc('font', family="Malgun Gothic")

data1 = open("data/자료/7차시/경주여행_지식인_2016_2.txt").read()
# 명사 추출
data2 = kkma.nouns(data1)
data3 = Counter(data2)
data3
# Counter({'제목': 1,
#          '경주': 2,
#          '여행': 1,
#          '문의': 1,
#          '문의드립': 1,
#          '드립': 1,
#          '5': 1,
#          '날': 1,
# ...
```

- 텍스트 파일에서 명사들을 추출하고 각 명사별로 빈도수 count

```python
# 불용어 제거
stop_words = open("data/자료/7차시/stop_words.txt").read()
data3 = [each_word for each_word in data2 if each_word not in stop_words]
data4 = []
for i in range(0, len(data3)):
    if len(data3[i]) >= 2 | len(data3[i]) <= 10:
        data4.append(data3[i])
```

- 미리 정의해놓은 불용어 파일을 불러와서 불용어가 아닌 단어들만 선별하고 2 ~ 10 글자의 단어들만 정제

```python
data5 = Counter(data4)
# 빈도수 상위 100개 단어
data6 = data5.most_common(100)
tmp_data = dict(data6)

# 한글은 폰트를 따로 설정해야함
wordcloud = WordCloud(font_path="C:\Windows\Fonts/H2GTRM.TTF",
                      relative_scaling=0.2, background_color="black").generate_from_frequencies(tmp_data)
plt.figure(figsize=(8, 4))
plt.imshow(wordcloud)
plt.axis("on")
plt.show()
```

<img src="https://user-images.githubusercontent.com/58063806/123268683-39880600-d539-11eb-939a-80bc1461463a.png" width=50% />

```python
from PIL import Image
from wordcloud import ImageColorGenerator
# mask image
alice_mask = np.array(Image.open("data/alice.png"))
wc = WordCloud(font_path="C:\Windows\Fonts/H2GTRM.TTF", mask=alice_mask, min_font_size=1, 
               max_font_size=40, max_words=2000, relative_scaling=0.2, 
               background_color="white").generate_from_frequencies(tmp_data)
plt.figure(figsize=(8, 13))
plt.imshow(wc)
plt.axis("off")
plt.show()
```

- mask image를 이용해 해당 모양으로 출력가능

<img src="https://user-images.githubusercontent.com/58063806/123268844-60463c80-d539-11eb-8eb6-da6759f427a1.png" width=25% />

```python
import nltk
from nltk.probability import FreqDist
plt.figure(figsize=(20, 4))
g_data4 = FreqDist(data4)
# 가장 많이 언급된 50개 단어만 
g_data4.plot(50)
```

- FreqDist : 단어를 키, 출현빈도를 값으로 하는 딕셔너리 형태와 유사

<img src="https://user-images.githubusercontent.com/58063806/123276349-0d23b800-d540-11eb-919e-9f7d24636d63.png" width=100% />

#### 영어

- nltk 라이브러리 이용

```python
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer, WordPunctTokenizer

data1 = open("data/자료/8차시/steve.txt").read()

# 영문 축약형까지 분리
tokenizer = WordPunctTokenizer()
# 형태소 분석
new_data2 = tokenizer.tokenize(data1)

# 영문 축약형 유지
# EX) "'You've"
tokenizer = RegexpTokenizer("[\w']+")
new_data3 = tokenizer.tokenize(data1)

# 불용어 제거
data4 = [each_word for each_word in new_data3 if each_word not in stopwords.words()]
data4
# ["'You've", 'got', 'find', 'love', "'", 'Jobs', 'says', 'This', 'text', ...

# 사전에 정의된 파일을 통해 추가적으로 불용어 제거
stop_words = open("data/자료/8차시/eng_stop_word.txt").read()

new_data4 = [each_word for each_word in data4 if each_word not in stop_words]
new_data5 = []
# 2 ~ 10자의 단어들만 정제
for i in range(0, len(new_data4)):
    if len(new_data4[i]) >= 2 | len(new_data4[i]) <= 10:
        new_data5.append(new_data4[i])
data5 = dict(Counter(new_data5).most_common(100))

wordcloud = WordCloud(relative_scaling=0.2, background_color="black").generate_from_frequencies(data5)
plt.figure(figsize=(10, 4))
plt.imshow(wordcloud)
plt.axis("on")
plt.show()
```

<img src="https://user-images.githubusercontent.com/58063806/123277355-00539400-d541-11eb-95f1-4da9905ee9b9.png" width=50%/>

```python
plt.figure(figsize=(10, 4))
g_data4 = nltk.Text(new_data5, name="다빈도 단어 그래프 출력")
g_data4.plot(50)
```

<img src="https://user-images.githubusercontent.com/58063806/123277607-3b55c780-d541-11eb-8e95-f3db8973f3c6.png" width=70% />