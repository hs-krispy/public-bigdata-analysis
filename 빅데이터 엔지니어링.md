## 빅데이터 엔지니어링

### 빅데이터 파이프라인



#### 1. 데이터 수집

**분석**

- 요구사항 파악
  - 분석 환경
  - 빅데이터 저장소
- 수집대상 파악
  - 데이터 유형
  - 데이터 위치
  - 수집 장애요소
  - 담당자 파악

**설계**

- 표준 수립 (여러 사람들 간의 협업을 위함)
  - 테이블 표준
  - 프로시저 표준
  - 네이밍 표준
  - 데이터 표준
- 매핑 정의서
  - 소스/타겟 데이터 타입
- 수집 정책
  - 수집 방식
  - 수집 주기
  - 시작 시간

**구현**

- Job 생성
  - 추출 Job 프로시저
- 워크플로우 생성
  - 선후행 연결
  - Timeout 설정
  - 실패시 액션

**테스트**

- Job 단위테스트
  - 에러 확인
  - 데이터 정합성 (중복, 누락된 데이터 확인)
- 워크플로우 테스트
  - 에러 로그
  - 배치 순서 확인
  - 배치 에러 대응 모니터링



### Data Warehouse (DW)

- 데이터를 주제별 다양한 각도로 분석하기 위해 기간계 시스템의 데이터를 ETL(Extraction Transformation Loading, **추출, 변환, 로딩**)하여 저장해 놓은 데이터베이스
- 빠른 의사결정을 위함 (의사결정자, C-Level의 임원)
- 100% 정형 데이터
- 스타 스키마로 테이블 구조를 변환



### Datalake

- **데이터 분석을 위해** 원천 시스템에 존재하는 **정형/비정형 데이터**를 **가공되지 않은 형태**로 보관해 놓은 단일 저장소
- 데이터 사이언티스트, 업무 부서내 분석가
- 데이터 검색
  - 기술 메타
  - 비즈니스 메타
- 데이터 저장소
  - 개별 시스템에서 분석 Needs가 높은 데이터를 미리 저장
  - 시스템 담당자와 협희 및 요청해야 할 시간 단축
- 분석환경 제공
  - R이나 파이썬 등을 통해 데이터레이크에 있는 데이터를 사용하여 분석할 수 있도록 환경 제공
- 시각화환경 제공
  - 일반적으로 분석환경 제공과 세트로 시각화 환경도 제공하며 빅데이터 분석 뿐만 아니라 데이터 결합을 통해서도 인사이트 획득 가능



#### 정형 데이터

- 고정된 스키마 구조가 있고 스키마 구조에 따라 데이터가 저장된 형태
- 일반적인 RDB, Excel 데이터
- ETL(Extract, Transformation, Load)

#### 반정형 데이터

- 스키마가 존재하지만 구조에 일관성이 없어서 데이터를 읽고 파싱해야 구조를 파악할 수 있는 데이터
- CSV, JSON, XML ...
- REST API, ETL

#### 비정형 데이터

- 스키마가 없는 데이터
- 정형/반정형 데이터 외의 데이터
- 크롤링, SFTP, FTP



#### DB 데이터 수집 유형

- DB 종속적 추출방식 
  - CDC 
    - 데이터베이스에 존재하는 트랜잭션 Log 파일을 읽어 타겟 저장소에 트랜잭션을 똑같이 반영하는 기술
- DB 독립적 추출방식
  - Timestamp 방식 
    - 수정 시간을 나타내는 컬럼을 읽어 마지막으로 반영했던 시간 이후분을 가져오는 방식
    - PK가 반드시 필요
    - 변경된 데이터를 가장 효율적으로 추출하는 방법
    - 원천에서 삭제되는 데이터는 반영이 불가능
  - Refresh 방식
    - 매번 전체 데이터를 새로 읽어 반영하는 방식으로 주로 소용량 테이블에 적용
    - 데이터 추출을 시도할 때마다 타겟 저장소의 테이블을 모두 비우고 원천 데이터 전체를 가져옴
    - 원천 테이블의 크기가 큰 경우 매번 추출하기에 시간이 오래 걸림 (주로 소규모 테이블에 사용)
  - Application Log 방식
    - 수집에 필요한 데이터만 별도 로그를 남기도록 어플리케이션 수정하는 방식
    - DB Trigger 방식 포함
- API 방식
  - REST API - PUT/POST/GET/DELETE를 이용하여 자원을 조회/수정/삽입 등이 가능한 API
    - HTTP 프로토콜 및 GET 메서드를 이용한 데이터 추출





#### 2. 데이터 정제

**분석**

- 대상 파악
  - 정제 대상 (정형/비정형)

- 권한 관리
  - 권한별 정책 (column, row 별로 권한 제어)

**설계**

- 데이터 프로파일링
  - 이상 데이터 확인 (결측, 이상치)
  - 개인정보 포함여부
- 데이터 결합 설계
  - 프로시저, View 설계

- 클렌징 준비
  - 클렌징 방식

**구현**

- 데이터 클렌징
  - 결측치 처리
  - 데이터 표준화
  - 비식별화
- 데이터 결합
  - 정형/반정형
- 데이터 제공
  - 분석환경(R, python)
  - 시각화 환경

**테스트**

- 워크플로우 테스트
  - 수집에서 제공까지 워크플로우 검증
- 정합성 검증
  - 데이터 정합성 검증



- 데이터 프로파일링
  - 테이블 및 컬럼 분석에 대한 통계정보를 확보하고 이를 통해 잘못된 값이나 모호한 의미를 가진 데이터를 발견하는 활동
- 데이터 표준화
  - 시스템에 산재된 단어와 이를 결합한 시스템 용어를 통일화하고 코드값, 도메인을 일치시켜 데이터 품질을 높이기 위한 활동
- 데이터 클렌징
  - 데이터에서 결측치, 이상값 등을 제거하고 표준화하여 데이터를 일관성있게 만드는 작업
- 데이터 검증
- 데이터 결합



#### 3. 데이터 분석

#### 4. 활용/시각화

